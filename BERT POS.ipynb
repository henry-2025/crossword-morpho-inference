{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ca0fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, AdamW\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc215bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model loading\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f89dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading, might take a bit\n",
    "import math\n",
    "SPLIT = 0.8\n",
    "\n",
    "# read in and unpack data, ignoring publisher, year, and answer fields\n",
    "data = csv.reader(open(\"./trainingdata/combined_2500.csv\", 'r'))\n",
    "data = [*data]\n",
    "random.shuffle(data)\n",
    "_, _, texts, _, tags = list(zip(*data))\n",
    "\n",
    "# get sorted list of unique PTB tags used in the data\n",
    "ptb_tags = list(set(tags))\n",
    "ptb_tags.sort()\n",
    "\n",
    "def tag_to_label(t):\n",
    "    return ptb_tags.index(t)\n",
    "\n",
    "def label_to_tag(l):\n",
    "    return ptb_tags[l]\n",
    "\n",
    "# convert tags to numbered labels\n",
    "labels = list(map(tag_to_label, tags))\n",
    "\n",
    "\n",
    "# train_texts, test_texts = list(text[:math.floor(SPLIT*len(text))]), list(text[math.floor(SPLIT*len(text)):])\n",
    "# train_labels, test_labels = list(labels[:math.floor(SPLIT*len(text))]), list(labels[math.floor(SPLIT*len(text)):])\n",
    "\n",
    "# generate train, test, validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2)\n",
    "\n",
    "# prepare token vectors\n",
    "train_encodings = tokenizer(train_texts, padding=True, truncation=True, return_tensors='pt')\n",
    "val_encodings = tokenizer(val_texts, padding=True, truncation=True, return_tensors='pt')\n",
    "test_encodings = tokenizer(test_texts, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af5dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating pytorch dataset interfaces\n",
    "import torch\n",
    "\n",
    "class CrossWordDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        item = {key: val[i].clone().detach() for key, val in self.encodings.items()}\n",
    "        print(item)\n",
    "        item['labels'] = torch.tensor(self.labels[i])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = CrossWordDataset(train_encodings, train_labels)\n",
    "val_dataset = CrossWordDataset(val_encodings, val_labels)\n",
    "test_dataset = CrossWordDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de57aa6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# download model\n",
    "model_type = \"bert-base-cased\" #could use \"distilbert-base-cased\"?\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_type, num_labels=len(ptb_tags), problem_type=\"single_label_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ee723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model with arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    log_level=\"warning\"\n",
    "    )\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c60f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model on input datasets\n",
    "# about 20 minutes for bert-base-cased\n",
    "\n",
    "train_output = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a5c056",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# use model to predict test set labels\n",
    "predicted = trainer.predict(test_dataset)\n",
    "predicted_labels = np.argmax(predicted.predictions, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d3c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate f1 scores for multiclass classification\n",
    "from sklearn.metrics import f1_score\n",
    "for av in [\"micro\", \"macro\", \"weighted\"]:\n",
    "    print(f\"F1 {av}: {f1_score(test_labels, predicted_labels, average=av)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f64bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from collections import Counter\n",
    "\n",
    "matrix_size = 5\n",
    "most_common = [label for (label, count) in Counter(predicted_labels).most_common(matrix_size)]\n",
    "disp_test, disp_predictions = [], []\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i] in most_common:\n",
    "        disp_test.append(most_common.index(test_labels[i]))\n",
    "    else:\n",
    "        disp_test.append(matrix_size+1)\n",
    "    if predicted_labels[i] in most_common:\n",
    "        disp_predictions.append(most_common.index(predicted_labels[i]))\n",
    "    else:\n",
    "        disp_predictions.append(matrix_size+1)\n",
    "        \n",
    "disp_tags = [label_to_tag(label) for label in most_common ] + ['other']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "disp = ConfusionMatrixDisplay.from_predictions(disp_test, disp_predictions, normalize='all', labels=list(np.arange(matrix_size+1)), display_labels=disp_tags, include_values=True, ax=ax)\n",
    "plt.savefig('confusion_matrix', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbf3f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance statistics for top 5 tags\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion = confusion_matrix(disp_test, disp_predictions)\n",
    "correct = np.diag(confusion)[:-2]\n",
    "fn = np.array(list(map(sum, confusion)))[:-2] # actually true negatives plus false positives\n",
    "fp = np.array(list(map(sum, confusion.T)))[:-2] # actually true positives plus false positives\n",
    "precision = correct/fp\n",
    "recall = correct/fn\n",
    "\n",
    "micro_precision = sum(correct)/sum(fp)\n",
    "micro_recall = sum(correct)/sum(fn)\n",
    "\n",
    "f1 = 2*precision*recall/(precision + recall)\n",
    "\n",
    "macro = sum(f1)/len(f1)\n",
    "micro = 2*micro_precision*micro_recall/(micro_precision + micro_accuracy)\n",
    "weighted = sum(f1*fp)/sum(fp)\n",
    "\n",
    "print('\\t   {:<5}{:<5}{:<5}{:<5}{:<5}'.format(*disp_tags[:-1]))\n",
    "print(\"Precision: {:.2f} {:.2f} {:.2f} {:.2f} {:.2f}\".format(*list(precision)))\n",
    "print(\"Recall:    {:.2f} {:.2f} {:.2f} {:.2f} {:.2f}\".format(*list(accuracy)))\n",
    "print(\"f1:        {:.2f} {:.2f} {:.2f} {:.2f} {:.2f}\\n\\n\".format(*list(f1)))\n",
    "print(\"averaged statistics for the top 5 classes:\\nmacro average:\", macro, \"\\nmicro average:\", micro, \"\\nweighted average:\",weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4229a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i] != predicted_labels[i]:\n",
    "        print(f\"Predicted {label_to_tag(predicted_labels[i])}, expected {label_to_tag(test_labels[i])}: {test_texts[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
