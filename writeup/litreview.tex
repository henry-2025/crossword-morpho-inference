%
% File acl2021.tex
%
%% Based on the style files for EMNLP 2020, which were
%% Based on the style files for ACL 2020, which were
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2021}
\usepackage{times}
\usepackage{latexsym}
\renewcommand{\UrlFont}{\ttfamily\small}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

\aclfinalcopy % Uncomment this line for the final submission
%\def\aclpaperid{***} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Final Project Lit Review}

\author{Miles Christensen \\
  Harvey Mudd College \\
  \texttt{mchristensen@hmc.edu} \\\And
  Henry Pick \\
  Harvey Mudd College \\
  \texttt{hpick@hmc.edu} \\}

\date{\today}

\begin{document}
\maketitle
% \begin{abstract}
% abstract
% \end{abstract}

\section{Related Work}

\subsection{Crossword solving}

Crossword solving as a problem in NLP has been studied for years. The first system designed to programmatically solve crosswords was Proverb \cite{Keim99-PROVERB, Littman02-AProbabilistic}, and two more modern state-of-the-art solvers are WebCrow \cite{Angelini05-Webcrow} and Dr. Fill \cite{Ginsberg11-DrFill}. Proverb and WebCrow adopt a modular approach for generating candidate answers: clues are fed to multiple different modules that each produce their own set of candidates, which are then merged to provide a full list of candidates for each clue. With these candidate answers, the task of filling the crossword grid can then be formalized as a probabilistic constraint satisfaction problem \cite{Shazeer99-Solving}. In comparison to Dr. Fill and WebCrow, Dr. Fill excels in optimizing solutions to this constraint satisfaction problem with various heuristics and post-processing algorithms. This, however, is outside the scope of our work, and we will focus primarily on the modules of Proverb ad WebCrow.

Many of Proverb's modules rely heavily on a crossword database (CWDB) of over 250,000 unique clue-answer pairs, finding exact matches, partial matches, or measuring the co-occurrence of words in clues and answers. Other modules are domain-specific (such as \verb.Movies. or \verb.Geography.) and query online sources (such as \verb,www.imdb.com, or the Getty Information Institute) to generate candidate answers. WebCrow also has a similar (but significantly smaller) CWDB, but it does not have domain-specific modules; most of its candidate generation is instead performed by a generic web search module. This module reformulates the clue into multiple search engine queries, downloads the resulting documents, then extracts and filters all words in these documents to find appropriate candidates.

\subsection{Morphological filtering}
Both WebCrow and Dr. Fill use filtering techniques to rank candidate results and remove others as they are being fetched from a knowledge databases. WebCrow's web search module, for example, uses a morphological filter, a scoring function that ranks words according to their morphological category. The filter first uses TreeTagger \cite{Schmid95-Improvements} to generate a part-of-speech (PoS) tag probability vector for each word in the clue, and then uses these vectors along with other features to classify the clue as a whole using a multi-class Kernel-based Vector Machine \cite{Tsochantaridis04-Support}. The classifier uses features that are present only in the clue and not in the crossword constraint (ie. number of words in the clue, length of the target answer, number of capital letters in the clue). Dr. Fill similarly recognizes that the clue and answer must follow the same morphological constraints, satisfying what is referred to as the ``substitution test''. For example, a clue for the answer \verb.SPAS. is ``Places for mani-pedis''; because the clue is plural, the answer must be as well, so they can be substituted for one another in the context of a sentence. 

In our project, we aim to recreate such classifiers using different techniques and evaluate their performance on a similar database of crossword clue-answer pairs.

\section{Methods}

\subsection{Dataset}
The dataset we will be using is a public dataset at \verb,xd.saul.pw,. It is a set of over 6,000,000 published crossword clue-answer pairs. Each pair has the clue, the answer, the publisher, and the year in which the puzzle was published. Because the dataset was developed independently for a personal project, there is no research paper describing it, but it is provided for public use under the MIT License.

\subsection{PoS Tagging Training Data}
The dataset of answer/clue pairs currently contains no PoS tags and is too large to tag manually, meaning one must resort to automatic taggers to create training data for our classifiers. We accomplish this through the standard SpaCy language \texttt{nlp} tagger on the Penn Treebank Tagset. Automatic tagging poses a challenge because it only infers the PoS tag from answer word itself without considering the clue in context. Inevitably, many answers will not have one dominant PoS tag, in which case we will use a subset of the most likely tags above a certain threshold and use each of these as an individual training example. For example, if the target is `throw' and the PoS tagger provides a probability of $[0.3, 0.7]$ for the tags \texttt{NN} and \texttt{VB}, our tagger would create two training examples, one in which the answer tag is certainly \texttt{NN} and another in which it is \texttt{VB}. % figure out which tagging model spacy uses and explain how to handle ambiguous predictions

\subsection{Multi-Class Support Vector Machines}
All modern comprehensive crossword solvers seem to make use of some classifier that uses simple features from each word clue to rank PoS tags. While no papers provide evidence as to the effectiveness of such classifiers, it seems to be a straightforward step in reducing the size of the list of candidate answers. Without an extensive database of general knowledge or the ability to query search engines, we cannot reasonably generate our own candidate lists. Our goal is simply to infer the PoS of the answer given some clue. We accomplish this through a Support Vector Machine which is frequently utilized in NLP classification problems on vector features such as this one. One SVM will use a linear kernel while another will use a RBF kernel, allowing us to assess whether the more complex model (the RBF) is more susceptible to being overtrained. Our feature vectors will consider a set similar to those used by WebCrow: the length of the target, the number of words in the clue, the number of capital letters in the clue, and the probability tag vectors for each word in the clue.

\subsection{Classification with Contextual Embeddings}
Designing a classifier that uses the contextual embeddings from a model like BERT \cite{Devlin-BERT} is a deserving consideration of our work. As opposed to the simple feature classifiers, a contextual embedding classifier based on BERT will consider the full semantic expression. The training of such a BERT classifier will follows the process of fine-tuning outlined in the original Devlin paper. This simply adds a classification layer to the pre-trained model, training the parameters on the clue-PoS pairs described above.

\bibliographystyle{acl_natbib}
\bibliography{litreview}


\end{document}